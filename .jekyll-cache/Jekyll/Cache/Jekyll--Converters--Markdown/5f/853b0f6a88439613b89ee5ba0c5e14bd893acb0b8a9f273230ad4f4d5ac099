I"Yt<p>Virtualization is method of logically dividing the system resources into virtualized resources and maps them to physical resources. This can be done either from Hardware <em>(Intel VT)</em> or Software <em>(Hypervisor)</em> an old idea developed by IBM in 60s and 70s.</p>

<h3 id="applications-of-virtualization">Applications of Virtualization:</h3>

<ul>
  <li>Resource utilization
    <ul>
      <li>Machines today are powerful, want to multiplex their hardware</li>
      <li>Can migrate VMs from one machine to another without shutdown</li>
    </ul>
  </li>
  <li>Software use and development
    <ul>
      <li>Can run multiple OSes simultaneously</li>
      <li>Can do system (e.g., OS) development at user-level</li>
    </ul>
  </li>
  <li>Many other cool applications
    <ul>
      <li>Debugging, emulation, security, speculation, fault tolerance…</li>
    </ul>
  </li>
  <li>Common theme is manipulating applications/services at the granularity of a machine
    <ul>
      <li>Specific version of OS, libraries, applications, Etc.</li>
    </ul>
  </li>
</ul>

<h3 id="the-golden-standards">The Golden Standards:</h3>

<p>In Popek and Goldberg’s 1974 paper they proposed three fundamentals characteristics for a software system to be called as a VMM.</p>

<ul>
  <li><strong>Fidelity:</strong> OS and applications should run without modifications.</li>
  <li><strong>Performance:</strong> Majority of the Guest OS operations should executed directly on the hardware.</li>
  <li><strong>Safety:</strong> The Guest OS shouldn’t take control over the system.</li>
</ul>

<h3 id="types-of-virtualization">Types of virtualization:</h3>

<ul>
  <li>Partitioning: CPU is divided into different parts and each part works as an individual system (eg: <strong>LPARs</strong>)</li>
  <li>Full Virtualization: Creating a virtual instance that is not aware of the fact that it’s virtualized.
    <ul>
      <li>Software Based: <strong>Binary Translation</strong></li>
      <li>Hardware Based: <strong>KVM</strong></li>
    </ul>
  </li>
  <li>Paravirtualization: If the Guest OS is aware of it being virtualized (Needs some OS level Modifications) <strong>XEN</strong></li>
  <li>Hybrid Virtualization: Paravirtualization + Full virtualization (<strong>ESXi</strong>, <strong>XEN</strong>)</li>
  <li>Container-Based virtualization: Container is an object that packages an application and all its dependencies (<strong>Docker</strong>)</li>
</ul>

<p>Full Virtualization(Binary Translation) , Paravirtualization(XEN), Hybrid Virtualization(KVM) are the widely used virtualization techniques currently. VMM plays a critical role in allocating resources to the Guest OSes ensuring different Guest get allocated as per their requirements. Let’s see some of these techniques in detail below.</p>

<p>VMMs or Hypervisor can be classified into two types depending on their placement, If an hypervisor is directly runs on top of the hardware then it is calld as <strong><code class="language-plaintext highlighter-rouge">Type 1 hypervisor</code></strong> else if their’s an OS between hardware and hypervisor then it is called as <strong><code class="language-plaintext highlighter-rouge">Type 2 hypervisor</code></strong></p>

<div class="mermaid">
graph TD
Type1-VM1--&gt;Type1-Hypervisor
Type1-VM2--&gt;Type1-Hypervisor
Type1-Hypervisor--&gt;Type1-Hardware
Type2-VM1--&gt;Type2-HypervisorSoftware
Type2-VM2--&gt;Type2-HypervisorSoftware
Type2-HypervisorSoftware--&gt;Type2-HostOS
Type2-HostOS--&gt;Type2-Hardware
</div>

<h1 id="adaptive-binary-translation-software-virtualization">Adaptive Binary Translation (Software Virtualization)</h1>

<p><u>Classical virtalization:</u> <strong>Trap and Emulate</strong> based technique which was widely popular in 1974, Doesn’t follow the golden standards of Popek and Goldberg as virtualization techniques were run on x86 which is not classically virtualizable. it also faced different obstacles for being completely virtualized such as:</p>

<ul>
  <li>Visibility of privilaged state.</li>
  <li>Lack of traps when privileged instructions run at user-level.</li>
</ul>

<p>Vmware proposed a solution for this problem to overcome the semantic obstacles faced by classical virtualization by adding an intrepter layer between the Guest OS and physical CPU and customize this interpreter to prevent the leakage of privilaged state and correctly implement non-trapping instructions <em>(popf)</em>.</p>

<h3 id="challenges-for-binarytranslation">Challenges for BinaryTranslation:</h3>

<ul>
  <li>De-Privileging guests.</li>
  <li>Protecting VMM from guest memory access</li>
  <li>Updating guest shadow data strutures.</li>
</ul>

<p><strong>Note:</strong> This approach is similar to running JVMs using JIT compilers.</p>

<p><img src="/assets/virtual/abt.png" style="zoom:80%;" /></p>

<p>Vmware proposed a translator with these properties that takes care of this challenges:</p>

<ul>
  <li><em>Binary</em>: Takes binary x86 code as input.</li>
  <li><em>Dynamic</em>: Translation happens at run-time.</li>
  <li><em>On Demand</em>: Code is translated only when it is about to execute.</li>
  <li><em>System Level</em>: It makes no assumptions on guest code. The VMM must run a buffer overflow that clobbers a return address.</li>
  <li><em>Subsetting</em>: It returns a safe subset of instructions for Host to perform.</li>
  <li><em>Adaptive</em>: Translated code adjusted in response to guest behavior.</li>
</ul>

<h4 id="how-does-translation-work">How does translation work?</h4>

<p>The binary translation is done dynamically, on-demand just before a chunk of code is executed, it is usually performed in small units called “basic blocks”. A basic block is a set of instructions that ends with a branch instruction but does not have any branch instructions inside. Such a block will always be executed start to finish by a CPU, and is therefore an ideal unit for translation. The translations of the basic blocks are cached. This means that the overhead of translating only happens the first time a block is executed.</p>

<p>To reduce the overhead of translating everything. The translated blocks are loaded at a different memory offset than the untranslated blocks, and are also usually larger than the original blocks. This means that both absolute and relative memory references need to be relinked in the translated code. Branch instructions at the end of a basic block can also be relinked to jump directly to another basic block. This is called “block chaining” and is an important performance optimization, But not all instructions can be translated such as</p>

<ul>
  <li>PC-relative addressing</li>
  <li>Direct control flow</li>
  <li>Indirect control flow</li>
  <li>Privilaged instructions</li>
</ul>

<p>While BT eliminates traps from privileged instructions it uses Adaptive Binary Translation to eliminate non-privileged instructions from accessing sensitive information such as page tables. by detecting instructions that trap frequently and adapt their translation.</p>

<blockquote>
  <p>Intel and AMD proposed new architectural changes that permit classical virtualization by introducing a new data structure called VMCB(virtual machine control block) which combines control state and subset state of guest virtual CPU. Which we will be discussing in detail when we analyze <strong>KVM</strong> which makes use of this new hardware changes.</p>
</blockquote>

<h3 id="recent-advances-and-important-features">Recent Advances and Important Features:</h3>

<ul>
  <li>Direct Execution: It’s runs guest user code in (CPL3) with some execptions for better efficiency and simplicity.</li>
  <li>Chaining Optimization to speed up inter-CCF(compiled code fragment) transfers.</li>
  <li>Heeds all of the golden standards. (Performance is not that good)</li>
  <li>Precise exceptions and trap elimination and callout avoidance some of the noteworthy features of VMware sofware virtualization solution.</li>
  <li>CPU-intensive apps: 2-10% overhead</li>
  <li>I/O-intensive apps: 25-60% overhead</li>
</ul>

<h1 id="xen-and-the-art-of-virtualization-paravirtualization">Xen and the Art of Virtualization (Paravirtualization)</h1>

<p>x86 Architecture made Virtualization very difficult as it was not designed with virtualization in mind. So it started to behave differently in kernal and user mode. so they have to come up with either full software emulation or binary translation. This made Xen a project at Cambridge computer laboratory to rethink about virtualization and come up with a new way which is now called as <strong><code class="language-plaintext highlighter-rouge">ParaVirtualization</code></strong> by,</p>

<ul>
  <li>Designing a new interface for virtualization.</li>
  <li>Allowing guests to collbrate in virtualizations.</li>
  <li>Providing new interface to guests to reduce overhead of virtualization.</li>
</ul>

<p>The hypervisor was first described in a SOSP 2003 paper called “<strong>Xen and the Art of Virtualization.</strong>” It was open sourced to allow a global community of developers to contribute and improve the hypervisor. The hypervisor remained an open source solution and has since become the basis of many commercial products.In 2013, the project went under The Linux Foundation. Accompanying the move, a new trademark “Xen Project” was adopted to differentiate the open source project from the many commercial efforts which used the older “Xen” trademark.</p>

<p>Xen still powers some of the largest cloud providers such as Amazon Web Services(AWS), Tencent, Alibaba Cloud, Etc. Although the current xen has been completed remolded by combinig the best features from Xen and HVM but it still follows the same core principles that they published in 2003 <a href="https://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf">paper</a>. by Univ of Cambridge computer laboratory. Xen proposes that to fully virtualize this kind of architectures we need to build a increased complexity and reduced performance virtual machines and argues that by allowing Guest OS access to the host OS we can better support time sensitive tasks and correctly handle TCP timeouts and RTT estimates which in return improves the performance of the virtual machines.</p>

<p>Xen showed that it was able to acheive all of this by introducing <code class="language-plaintext highlighter-rouge">Hypervisior</code> an abstarct machine on top on the host H/W which can avoid the difficulties of virtualizing all parts of the architecture. This is also called as <strong><code class="language-plaintext highlighter-rouge">ParaVirtualization</code></strong> .</p>

<h3 id="design-principles">Design Principles:</h3>

<ul>
  <li>Virtualization should allow isolation of processes and virtual machines.</li>
  <li>It should be able to support variety of Operating systems.</li>
  <li>Performance overhead between Host and Guest OS should be minimum.</li>
</ul>

<p><strong>Note:</strong> Although you need modify Guest OS a bit, you don’t need to modify Guest applications.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Memory Management</th>
      <th style="text-align: center"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Segmentation</td>
      <td style="text-align: center">Cannot install fully-privileged segment descriptors and cannot overlap with the top end of the linear address space.</td>
    </tr>
    <tr>
      <td style="text-align: center">Paging</td>
      <td style="text-align: center">Guest OS has direct read access to hardware page tables, but updates are batched and validated by the hypervisor. A domain may be allocated discontiguous machine pages.</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>CPU</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Protection</td>
      <td style="text-align: center">Guest OS must run at a lower privilege level than Xen.</td>
    </tr>
    <tr>
      <td style="text-align: center">Exceptions</td>
      <td style="text-align: center">Guest OS must register a descriptor table for exception handlers with Xen. Aside from page faults, the handlers remain the same</td>
    </tr>
    <tr>
      <td style="text-align: center">System Calls</td>
      <td style="text-align: center">Guest OS may install a ‘fast’ handler for system calls, allowing direct calls from an application into its guest OS and avoiding indirecting through Xen on every call.</td>
    </tr>
    <tr>
      <td style="text-align: center">Interrupts</td>
      <td style="text-align: center">Hardware interrupts are replaced with a lightweight event system</td>
    </tr>
    <tr>
      <td style="text-align: center">Time</td>
      <td style="text-align: center">Each guest OS has a timer interface and is aware of both ‘real’ and ‘virtual’ time.</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Device I/O</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Network, Disk, etc.</td>
      <td style="text-align: center">Virtual devices are elegant and simple to access. Data is transferred using asynchronous I/O rings. An event mechanism replaces hardware interrupts for notifications.</td>
    </tr>
  </tbody>
</table>

<center>Table: The para-virtualized x86 interface</center>

<h3 id="the-xen-vm-interface">The XEN VM Interface:</h3>

<p><img src="/assets/virtual/xenarch.png" alt="Architecure of Xen" style="zoom:67%;" /></p>

<h4 id="memory-management">Memory Management:</h4>

<p>As x86 doesn’t support Software managed TLB (Translation Lookaside Buffer), Xen allows Guest OS to create their own page tables by giving read only access to the hardware page tables.</p>

<ul>
  <li>Thus to avoid TLB flush Xen exists in a 64MB section at the top of every address space, every time Guest OS requires to update the page tables or need write access to the page tables all this transactions are validated by Xen to ensure safety and isolation.</li>
  <li>This level segmentation is acheived by giving Guest OS a lower privilege than Xen and not modification accces to Xen address space.</li>
</ul>

<h4 id="cpu">CPU:</h4>

<ul>
  <li>Hypervisor creates an illusion that the virtual machine owns a set of CPUs and Memory with the host.</li>
  <li>Guest OS is modified to run in a lower privileage level than Host OS so any guest OS attempt to executes a privileaged instruction it is failed by the processor either silently or by taking a fault, since only XEN executes at a sufficient privileage level.</li>
  <li>Most of exceptions such as memory faults and software faults are similar to x86, the only type of instructions that effect the system performance are page faults and system calls
    <ul>
      <li>System calls can be taken care effectively by aloowing each Guest OS to register a fast exception handler.</li>
      <li>Since same approach is not possible for page faults therefore there are always be delivered via Xen so that this register value can be saved for access in ring 1.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Double faults</code> results in termination of Guest OS.</li>
</ul>

<h4 id="device-io">Device I/O:</h4>

<ul>
  <li>To this end, I/O data is transferred to and from each domain via Xen, using shared-memory, asynchronous bufferdescriptor rings</li>
  <li>When Xen boots up, it launches Dom0, the first privileged domain <em>(Linux 2.6)</em>, but in theory can be any other OS that has been properly modified</li>
  <li>Dom0 is a privileged domain that can touch all hardware in the system</li>
  <li>Dom0 exports some subset of the the devices in the system to the other domains, based on each domain’s configuration</li>
  <li>The devices are exported as “class devices”, e.g. a block device or a network device.</li>
</ul>

<h3 id="core-functionalities">Core Functionalities:</h3>

<ul>
  <li>Xen uses BVT algorithm for CPU scheduling (with option to control)</li>
  <li>Guest Os is aware of both real time, virual time and wall-clock time which can used for Guest OS for better task scheduling.</li>
  <li>Most of work only includes replacing privilaged instruction with hyper calls.</li>
  <li>Page tables of Guest OS are preregistered with MMU, and also handles the exceptions in bulk to reduce the overhead.</li>
  <li>Ballon Driver technique is used to allocate used memory or extra meory of different VMs via XEN.</li>
  <li>Disk access is done via VBD (vistual block devices) which are maintained by Domain 0.</li>
  <li>Round robin used to implement packet scheduler.</li>
</ul>

<h3 id="recent-advances-and-important-features-1">Recent Advances and Important Features:</h3>

<ul>
  <li>Requires <strong>minimal</strong> operating systems changes, and no userspace changes</li>
  <li>Provides secure isolation, resource control and QoS</li>
  <li>Close to native performance!</li>
  <li>Supports live migration of VMs</li>
  <li>Currently XEN was modified to use features from XEN and HVM for better performance.</li>
</ul>

<h1 id="kvm-the-linux-virtual-machine-monitor-hardware-virtualization">kvm: the Linux Virtual Machine Monitor (Hardware Virtualization)</h1>

<p>After a lot of virtualization papers starting their publications with x86 doesn’t support virtualization (NOT TRUE). Hardware manufactures such Intel and AMD started to virtualization extensions to x86 processors that can be used towrite relatively simple VMMs. Kvm is one of them which is a Kernel-based Virtual Machine for Linux system to run multiple os on top of them as a normal linux processes.</p>

<p><img src="/assets/virtual/kvm.png" style="zoom:67%;" /></p>

<h3 id="new-hardware-features">New Hardware Features:</h3>

<ul>
  <li>A new Guest Operating mode: A less privileaged level</li>
  <li>Hardware state switch: Hardware switches for guest and host mode</li>
  <li>Exit reason reporting: Error/Reason tracking while switching modes</li>
</ul>

<h3 id="how-does-kvm-work">How does KVM work?</h3>

<ul>
  <li><strong>Start KVM</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">/dev/kvm</code> exposes a device node used by userspace to create and run VMs through set of <code class="language-plaintext highlighter-rouge">ioctl()</code>s.</li>
      <li>Operation provided are:
        <ul>
          <li>Creation of new VM</li>
          <li>Allocation of memory</li>
          <li>R/W to virtual cpu registers</li>
          <li>Injecting an interrupt into a virtual cpu.</li>
          <li>Running a virtual CPU.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Memory Management:</strong>
    <ul>
      <li>The qemu/kvm process runs mostly like a normal Linux program. It allocates its memory with normal malloc() or mmap() calls. If a guest is going to have 1GB of physical memory, qemu/kvm will effectively do a malloc(1«30), allocating 1GB of host virtual space. However, just like a normal program doing a malloc(), there is no actual physical memory allocated at the time of the malloc(). It will not be actually allocated until the first time it is touched. Once the guest is running, it sees that malloc()’d memory area as being its physical memory. If the guest’s kernel were to access what it sees as physical address 0x0, it will see the first page of that malloc() done by the qemu/kvm process.</li>
      <li>It used to be that every time a KVM guest changed its page tables, the host had to be involved. The host would validate that the entries the guest put in its page tables were valid and that they did not access any memory which was not allowed.</li>
    </ul>
  </li>
  <li><strong>CPU Management:</strong>
    <ul>
      <li>At the outermost level, userspace calls the kernel to execute guest code until it encounters an I/O instruction, or until an external event such as arrival of a network packet or a timeout occurs. External events are represented by signals.</li>
      <li>At the kernel level, the kernel causes the hardware to enter guest mode. If the processor exits guest mode due to an event such as an external interrupt or a shadow page table fault, the kernel performs the necessary handling and resumes guest execution. If the exit reason is due to an I/O instruction or a signal queued to the process, then the kernel exits to userspace.</li>
      <li>At the hardware level, the processor executes guest code until it encounters an instruction that needs assistance, a fault, or an external interrupt.</li>
    </ul>
  </li>
</ul>

<h3 id="io-virtualization">I/O virtualization:</h3>

<ul>
  <li>KVM uses a kernel module to intercept I/O requests from a Guest OS, and passes them to QEMU, an emulator running on the user space of Host OS.</li>
  <li>kvm also provides a mechanism for userspace to inject interrupts into the guest.</li>
  <li>In order to efficiently support framebuffers, kvm allows mapping non-mmio memory at arbitrary addresses such as the pci region.</li>
</ul>

<h3 id="mmu-virtualization">MMU virtualization:</h3>

<ul>
  <li>The actual set of page tables being used by the virtualization hardware are separate from the page tables that the guest <em>thought</em> were being used. The guest first makes a change in its page tables. Later, the host notices this change, verifies it, and then makes a real page table which is accessed by the hardware. The guest software is not allowed to directly manipulate the page tables accessed by the hardware. This concept is called shadow page tables.</li>
  <li>The VMX/AMD-V extensions allowed the host to trap whenever the guest tried to set the register pointing to the base page table (CR3).</li>
  <li>In order to improve guest performance, the virtual mmu implementation was enhanced to allow page tables to be cached across context switches. This greatly increases performance at the expense of much increased code complexity.</li>
</ul>

<h3 id="recent-advances-and-important-features-2">Recent Advances and Important Features:</h3>

<ul>
  <li>Both AMD and Intel sought solutions to these problems and came up with similar answers called EPT and NPT. They specify a set of structures recognized by the hardware which can quickly translate guest physical addresses to host physical addresses <em>without</em> going through the host page tables. This shortcut removes the costly two-dimensional page table walks.</li>
  <li>Improved utilization of resources and access to it by providing flexible storage</li>
  <li>A virtual machine created by it is a standard Linux process, scheduled by its native standard Linux scheduler.</li>
  <li>Leverages the capability of MMU (Memory management unit) in hardware to virtualize the memory with improvement in performance.</li>
</ul>

<h1 id="comparsions-between-different-virtualizations">Comparsions Between Different Virtualizations:</h1>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Paravirtualization (XEN)</th>
      <th style="text-align: center">Full Virtualization (ABT)</th>
      <th style="text-align: center">Hardware Virtualization (KVM)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Doesn’t provide complete isolation of Guest OS</td>
      <td style="text-align: center">Provides complete Isolation of Guest OS</td>
      <td style="text-align: center">Provides complete Isolation of Guest OS</td>
    </tr>
    <tr>
      <td style="text-align: center">Requires minimal OS modifications according to XEN.</td>
      <td style="text-align: center">Provides Execution of running unmodified Operating systems</td>
      <td style="text-align: center">Provides Execution of running unmodified Operating systems</td>
    </tr>
    <tr>
      <td style="text-align: center">More Secure</td>
      <td style="text-align: center">Less Secure compared to Paravirtualization</td>
      <td style="text-align: center">More secure</td>
    </tr>
    <tr>
      <td style="text-align: center">Requires hypercalls for execution.</td>
      <td style="text-align: center">Requires Binary Translation for operations</td>
      <td style="text-align: center">Doesn’t require any translations or hypercalls</td>
    </tr>
    <tr>
      <td style="text-align: center">Less Portable and compatiable</td>
      <td style="text-align: center">Easily portable and compatibility</td>
      <td style="text-align: center">Easily portable and compatibility</td>
    </tr>
    <tr>
      <td style="text-align: center">Faster as most of the operations are run directly on Hardware</td>
      <td style="text-align: center">Slower compared to Paravirtualization.</td>
      <td style="text-align: center">Directly propotional to number of exits occured during execution.</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>

<h3 id="cpu-virtualization">CPU Virtualization:</h3>

<table>
  <thead>
    <tr>
      <th>Paravirtualization (XEN)</th>
      <th>Full Virtualization (ABT)</th>
      <th>Hardware Virtualization (KVM)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/assets/virtual/xencpu.png" alt="" /></td>
      <td><img src="/assets/virtual/abtcpu.png" alt="" /></td>
      <td><img src="/assets/virtual/kvmcpu.png" alt="" /></td>
    </tr>
    <tr>
      <td>The value proposition of paravirtualization is in lower virtualization overhead, but the performance advantage of paravirtualization over full virtualization can vary greatly depending on the workload.</td>
      <td>The combination BT and Direct execution provides Full virtualization to run  user level intructions at native speed and OS instructions by translating them on the fly and cache for further use.</td>
      <td>As shown in the figure, privileged and sensitive calls are set to automatically trap to the hypervisor, removing the need for either binary translation or paravirtualization.</td>
    </tr>
  </tbody>
</table>

<h3 id="memory-virtualization">Memory Virtualization:</h3>

<p><img src="/assets/virtual/memvir.png" alt="" /></p>

<p>As shown in the figure, To run multiple virtual machines on a single system, another level of memory virtualization is required. In other words, one has to virtualize the MMU to support the guest OS. The guest OS continues to control the mapping of virtual addresses to the guest memory physical addresses, but the guest OS cannot have direct access to the actual machine memory.</p>

<p>Most of the virtualization techniques uses shadow page tables to accelerate the mappings of guest to host memory addresses.</p>

<h3 id="io-virtualization-1">I/O Virtualization:</h3>

<p><img src="/assets/virtual/io.png" style="zoom:60%;" /></p>

<p>I/O Virtualization technique involves sharing a single I/O resource among multiple virtual machines. Approaches include hardware based, software based, and hybrid solutions.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">XEN I/O</th>
      <th style="text-align: center">KVM I/O</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/virtual/xenio.png" style="zoom:80%;" /></td>
      <td style="text-align: center"><img src="/assets/virtual/kvmio.png" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>In Full virtualization The real devices are emulated for I/O virtualization.</p>

<p>In Para virtualization: The devices are exported as <code class="language-plaintext highlighter-rouge">class devices</code>, e.g. a block device or a network device, not as a specific Hardware model.</p>

<h3 id="kvm-vs-xen-referred-from-a-quantitative-comparison-between-xen-and-kvm">KVM vs XEN: Referred from <a href="https://iopscience.iop.org/article/10.1088/1742-6596/219/4/042005/pdf">A quantitative comparison between xen and kvm</a></h3>

<p>During their tests kvm proved great stability and reliability: it never crashed and integrated seamlessly into our computing farm, without requiring any additional effort to the system administrators.</p>

<p>Their benchmarks showed that the CPU performance provided by the virtualization layer is comparable to the one provided by xen and in some cases it’s even better. Network performance is fair, showing some strange asymmetric behavior, but anyway we consider them acceptable.</p>

<p>Disk I/O seems to be the most problematic aspect, providing the VM poor performance, particularly when multiple machines concurrently access the disk. Anyway even xen based VM showed poor performance with this parameter, maybe caused by the solution adopted in our center, that is to provide virtual disks on a file.</p>

<p>To summarize, we can say that even if looking very promising, right now, xen hypervisor seems to be the best solution, particularly when using the para-virtualized approach.</p>

<h3 id="hardware-vs-software-virtualization">Hardware Vs Software Virtualization:</h3>

<p><strong>Note:</strong> Most of the experiments put forth by the paper are done on 1st gen Hardware assisted processers which are not the same with recent developments</p>

<ul>
  <li>Software and Hardware VMMs both perform well on compute-bound workloads.</li>
  <li>For workloads that perform I/O, create processes, or switch contexts rapidly, software outperforms hardware.</li>
  <li>In two workloads rich in system calls, the hardware VMM prevails.</li>
</ul>

<p><strong>Each virtualization technologies have their own advantages and disadvantages. The choice of virtualization heavily depends on use and cost.</strong></p>
:ET