I"oL<h2 id="tricks-to-improve-your-random-forests">Tricks to Improve your Random Forests</h2>

<!-- ![Trees](/assets/images/mountains.jpg) -->

<p><strong>Inspiration</strong>: <a href="http://course18.fast.ai/lessonsml1/lessonsml1.html">Jeremy Howard fastai Course</a></p>

<blockquote>
  <p>Whether you need to classify an object or predict a continues outcome or even a cluster unsupervised data Random Forests offers you a base line model to test your results sometimes even a better one than the most popular algorithm.</p>
</blockquote>

<p>With some of the perks including:</p>

<ul>
  <li>Parallelization</li>
  <li>Robustness</li>
  <li>Easy to Understand</li>
  <li>No need to assume anything about the data.</li>
  <li>Doesn’t overfit usually.</li>
  <li>Gives estimate of important varibales.</li>
  <li>No need for cross validation.</li>
</ul>

<blockquote>
  <p><em>“Come to woods, for here is the rest”</em>   - John Muir</p>
</blockquote>

<h5 id="algorithms-such-as-xgboost--random-forests-helped-many-to-land-top-on-the-winner-boards-in-many-machine-learning-competitions">Algorithms such as <strong>XGBoost</strong>  <strong>Random Forests</strong> helped many to land top on the winner boards in many machine learning competitions.</h5>

<p>Forest is nothing but a piece of land with many trees, which is same as a rondom forest which is nothing but a combination of many desicion trees. A slight analogy of differences between a crop and a forest can used to describe a good random forest model, i.e a good random forest is defined as the one with multiple trees which has less correlation between them.</p>

<p>​                                                     <strong>Here is the original paper on <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">Random Forests</a></strong></p>

<blockquote>
  <p><strong><em>You have fitted your data and claluclated the R^2 score of your model and got 0.75 what to do next to increase it to 0.98</em></strong></p>
</blockquote>

<p>Let’s write some helper functions that are useful in the process All the values you see below is for <a href="https://archive.ics.uci.edu/ml/datasets/iris">iris</a> dataset</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td><td class="rouge-code"><pre><span class="c1"># Import Libraries
</span><span class="kn">from</span> <span class="nn">fastai.imports</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai.structured</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="c1"># Load Dataset
</span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>

<span class="c1"># Split Data
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">x_train</span><span class="p">,</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Load Model
</span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Metrics Functions
</span><span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> <span class="k">return</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">print_score</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
  <span class="s">"""
  Returns: rmse of train, rmse of test, r^2 of train, r^2 of test and oob_score(if given)
  """</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">rmse</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">rmse</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">),</span>
                <span class="n">m</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">m</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)]</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s">'oob_score_'</span><span class="p">):</span> <span class="n">res</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">oob_score_</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="tip-1--grow-all-the-trees-at-the-same-time">Tip 1:  Grow all the trees at the same time:</h3>

<p>Random Forest models are very fast to train because of their parallelization across all the avaiable cores either in your CPU using</p>

<p><code class="language-plaintext highlighter-rouge">n_jobs</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># parallelize your Model
</span><span class="n">rf_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="c1"># Uses all the cores of your CPU
</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Uses 1 core in your cpu
</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># Uses 2 cores in your cpu etc.
</span></pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="tip-2-divide-the-tree-into-groups-to-help-them-grow-fast">Tip 2: Divide the tree into groups to help them grow fast:</h3>

<p>Random forests works perfectly even if you have billions of rows if you have enough resource and processing power <strong>OR</strong> you can break down data into subsets  -  <strong><em>SUB SAMPLING</em></strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="c1"># If you are using fastai library split_vals can be used to split data into subsets 
</span><span class="n">size</span> <span class="o">=</span> <span class="nb">int</span> <span class="c1"># required size
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">split_vals</span><span class="p">(</span><span class="n">df_trn</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> 
<span class="n">y_train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">split_vals</span><span class="p">(</span><span class="n">y_trn</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

<span class="c1"># set model samples
</span><span class="n">set_rf_samples</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>You can also <code class="language-plaintext highlighter-rouge">Bootstrap</code>  parameter to choose the samples from data</p>

<p><code class="language-plaintext highlighter-rouge">True</code> : Draws a subset sample size from the data with replacement</p>

<p><code class="language-plaintext highlighter-rouge">False</code> : Let’s you sample whole data for every tree while you the randomizes the columns that goes to the tree.</p>

<h3 id="tip-3-always-use-optimal-number-of-trees-for-a-better-forest">Tip 3: Always use optimal number of trees for a better Forest:</h3>

<p>Play with numbers of tree (<code class="language-plaintext highlighter-rouge">n_estimators</code>) until you get a good model or use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV">Grid Search</a> to find optimal parameters</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="n">m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">m</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">print_score</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="c1"># [0.09128709291752768, 0.18257418583505536, 0.9916666666666667, 0.9666666666666667]
</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">m</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">print_score</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="c1"># [0.12909944487358055, 0.18257418583505536, 0.9833333333333333, 0.9666666666666667]
</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">m</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">print_score</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="c1"># [0.0, 0.18257418583505536, 1.0, 0.9666666666666667]
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>Bagging</strong> : Combination of weak trees makes a forest a better classifier.</p>

<p>Plot the graph of tree vs score to get a better idea to select the number of estimators:</p>

<p><img src="/assets/images/RandomForest/Bagging.png" alt="Bagging" /></p>

<p>You will get a certain number where your score doesn’t get better, It good to stop there and experiment with other parameters.</p>

<h3 id="tip-4-what-if-new-trees-as-not-growing-as-good-as-old-ones">Tip 4: What if new trees as not growing as good as old ones:</h3>

<p>If your validation score is worse than training score their is a better way to check it rather than checking on validation data.</p>

<p>While implementing a random forest tree each tree uses only ceratin amount of data randomly to form a tree later we can use the remaining data to check if our model overfits or not this method by setting <code class="language-plaintext highlighter-rouge">oob_score</code> parameter to <code class="language-plaintext highlighter-rouge">True</code> in the model .</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">m</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">print_score</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="c1"># [0.0, 0.18257418583505536, 1.0, 0.9666666666666667, 0.95]
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>Here the oob_score is 0.95 while train_score is 0.1 while tells that data didn’t overfit that much.</p>

<h3 id="tip-5-get-the-satellite-veiw-of-your-forest">Tip 5: Get the satellite veiw of your forest:</h3>

<p>Its always a good way to visualize your forest model for better optimization, Todo that always use less estimators may be 1</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="n">m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bootstrap</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">m</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">print_score</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="c1"># Give representative Columns to your data.
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'sepal length'</span><span class="p">,</span> <span class="s">'sepal width'</span><span class="p">,</span> <span class="s">'petal length'</span><span class="p">,</span> <span class="s">'petal width'</span><span class="p">])</span>

<span class="c1"># Draw_tree function in fastai let's you visualize the tree  
</span><span class="n">draw_tree</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/images/RandomForest/tree.png" alt="tree diagram" /></p>

<p>We can observe here that <code class="language-plaintext highlighter-rouge">sepal length</code> plays an important role in differentiating the data</p>

<p><code class="language-plaintext highlighter-rouge">gini</code> : represents the error score if you classify data with that particular set of labels until then.</p>

<p><code class="language-plaintext highlighter-rouge">samples</code> : No:of samples in that group</p>

<p><code class="language-plaintext highlighter-rouge">value</code> : mean value of each class</p>

<h3 id="tip-6-less-branches-makes-the-tree-grow-tall">Tip 6: Less Branches makes the tree grow Tall</h3>

<p>Inorder to acheive this we will use:</p>

<p><code class="language-plaintext highlighter-rouge">min_samples_leaf</code>  :  This will limit the number of leaf nodes in a tree which intern reduces overfitting and therefore predictions are made using more number of estimators reducing volatility of the model.</p>

<p><code class="language-plaintext highlighter-rouge">max_features</code>  : Along with randomizing the data row-wise for each tree max_features limits the number of features for each tree ,which randomizes the model further and reduces bias.</p>

<p><img src="/assets/images/RandomForest/max_features.png" alt="Sklearn" /></p>

:ET